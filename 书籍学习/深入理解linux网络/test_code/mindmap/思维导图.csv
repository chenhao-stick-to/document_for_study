《深入理解Linux网络》,第2章 内核是如何接收网络包的,Linux启动准备,创建ksoftirqd内核线程,
,,,网络子系统初始化，为RX_SOFTIRQD和TX_SOFTIRQD注册处理函数,
,,,网卡驱动初始化，注册NAPI 要用的 poll 函数,
,,,启动网卡，并为网卡创建RingBuffer,
,,迎接数据的到来,网卡将数据包DMA到RingBuffer中,
,,,发起硬中断通知，CPU进入硬中断处理，处理完后发起软中断,
,,,ksoftirq内核线程开始进入软中断处理函数,
,,,将接收到的数据放到相应socket的接收队列中,
,,,通过同步阻塞或者epoll等方式和用户进程进行协作,
,第3章 内核是如何与用户进程协作的,同步阻塞,用户进程,创建socket对象（通过socket系统调用、或者accept接收）
,,,,调用recvfrom系统调用接收数据
,,,,到socket接收队列中查看是否有数据，有则接收并返回
,,,,如果数据未到达，则将当前进程阻塞，让出CPU
,,,硬件与内核处理,网卡将数据包DMA到RingBuffer中
,,,,发起硬中断通知，CPU进入硬中断处理，处理完后发起软中断
,,,,ksoftirq内核线程开始进入软中断处理函数
,,,,将接收到的数据放到相应socket的接收队列中
,,,,查找到socket等待队列上阻塞的进程，并唤醒将其推入运行队列
,,多路复用之epoll,用户进程,调用epoll_create创建epoll对象
,,,,通过epoll_ctl为该epoll添加要管理的socket，可以添加很多个，注册ep_poll_callback回调
,,,,调用epoll_wait等待所管理的socket上的可读、可写等事件
,,,,如果有事件需要被处理，则一直处理
,,,,直到没有事件需要处理，或者时间片到达，则让出CPU
,,,硬件与内核处理,网卡将数据包DMA到RingBuffer中
,,,,发起硬中断通知，CPU进入硬中断处理，处理完后发起软中断
,,,,ksoftirq内核线程开始进入软中断处理函数
,,,,将接收到的数据放到相应socket的接收队列中
,,,,调用socket等待队列项里注册的ep_poll_callback函数
,,,,将socket相关的epitem添加到就绪队列中（等待用户进程来取）
,,,,看一下epoll对象上有没有进程在阻塞，没有则不用管，有的话唤醒一下
,第4章 内核是如何发送网络包的,Linux网卡启动,创建RingBuffer环形队列,
,,用户进程发送操作：主要消耗CPU的sy时间,调用send后陷入内核态,
,,,将要发送的数据拷贝到内核态,
,,,判断是否需要立即发送，若不需要立即发送直接返回了就,
,,,如果需要立即发送则在协议栈中进行TCP、IP等包头的封装,
,,,在IP层进行路由项查找，判断要使用哪个Iface、哪个Gateway发送出去,
,,,邻居子系统自动获取下一跳mac地址并继续发送,
,,,通过网络设备子系统将包添加到发送队列真正发出,
,,,自己发送，或者可能触发软中断来发送,
,,软中断的发送：当用户进程系统态CPU用户时间用尽，软中断会发送，会消耗CPU的si时间,获取发送队列,
,,,调用网卡驱动，例如igb_xmit_frame进行发送,
,,发送RingBuffer内存回收,网卡发送完成后触发硬中断,
,,,硬中断触发NET_RX_SOFTIRQ类型软中断,
,,,进入网卡驱动的回调igb_poll、igb_clean_tx_irq,
,,,清理skb，解除skb映射,
,第5章 深度理解本机网络IO,本机发送过程,调用send后陷入内核态,
,,,将要发送的数据拷贝到内核态,
,,,判断是否需要立即发送，若不需要立即发送直接返回了就,
,,,如果需要立即发送则在协议栈中进行TCP、IP等包头的封装,
,,,在IP层进行路由项查找，判断要使用loopback设备发送,
,,,将要发送的数据入队（等待软中断来接收）,
,,本机接收过程,软中断启动后，调用回环设备的poll函数process_backlog,
,,,直接从队列上取下skb数据包,
,,,将数据送往协议栈进行处理,
,,,处理完后放入socket接收队列,
,第6章 深度理解TCP连接建立过程,深入理解listen,listen主要工作就是创建半连接、全连接队列,
,,,"全连接队列长度 = min(backlog, net.core.somaxconn)",
,,,半连接队列溢出通过打开syn_cookie来解决即可,
,,深入理解connect,选择一个可用端口,
,,,如果端口未被使用，则选择并返回,
,,,如果端口在用，但存在的连接和欲建立的连接四元组并不完全一致，则该端口仍然可用,
,,完整TCP连接建立过程,客户端发出第一次握手,客户端调用connect后陷入内核态
,,,,选择一个可用的端口
,,,,启动超时重传定时器（2.6内核中是3秒，3.10中是1秒）
,,,,发出syn握手包
,,,服务器响应第一次握手,服务器收到第一次握手包
,,,,判断半连接队列是否满了，满了且没开syn_cookie就丢弃
,,,,将该握手请求标记到半连接队列中（或syncookie计算）
,,,,判断全连接队列是否满，满的话丢弃
,,,,启动超时重传定时器
,,,,发出synack第二次握手包
,,,客户端响应第二次握手,客户端收到服务器的synack
,,,,清除connect时设置的重传定时器
,,,,修改自己的socket状态为ESTABLISH，打开保活定时器
,,,,发出第三次握手包
,,,服务器响应第三次握手,服务端收到第三次握手包
,,,,创建一个新的子sock对象出来
,,,,删除相应的半连接队列元素
,,,,添加新sock对象到全连接队列
,,,,等待用户进程来accept把连接取走
,,常见异常TCP连接异常 ,connect选择端口时端口不够用，导致CPU消耗上涨,保持充足可用的端口范围
,,,,使用长连接来减少频繁的握手
,,,握手丢包导致超时重传发生，伴随而来的就是秒级的响应延迟,加大全半连接队列
,,,,打开syn_cookie
,,,,用户进程尽早accept
,,,,调整重试参数包括net.ipv4.tcp_syn_retries和net.ipv4.tcp_synack_retries
,,,,使用长连接来减少频繁的握手
,第7章 一条TCP连接消耗多大的内存,Linux 内核如何管理内存,第一步：将内存条和CPU划分为多个node,
,,,第二步：把每个node划分成不同的zone，每个zone下都有许多page（4KB大小内存页）,
,,,第三步：每个zone下的pages通过伙伴系统来进行管理,
,,,第四步：slab机制来管理所有内核对象（slab向伙伴系统申请可用pages）,
,,TCP 连接相关内核对象开销：总共大约3.3KB左右,,
,第8章 一台机器最多能支持多少条TCP连接,Linux最大文件描述符限制,系统级别：fs.file-max,
,,,进程级别：fs.nr_open,
,,,用户进程级别：soft nofile,
,,客户端突破65535端口号限制方法,一台linux上可以配置多个ip,
,,,如果不配置多ip，那么一个端口也可以用于连接不同的server,
,第9章 网络性能优化建议,网络请求优化,尽量减少不必要的网络IO,
,,,尽量合并网络请求,
,,,调用者与被调用机器尽可能部署的近一些,
,,,内网调用不要使用外网域名,
,,接收过程优化,调整网卡RingBuffer大小,
,,,多队列网卡RSS调优,
,,,硬中断合并,
,,,软中断budget调整,
,,,接收合并处理,
,,发送过程优化,控制数据包大小,
,,,减少内存拷贝次数,
,,,推迟分片,
,,,多队列网卡XPS调优,
,,,本机网络IO下使用eBPF绕开协议栈发送接收开销,
,,内核与进程协作方式优化,少用同步阻塞的编程模型,
,,,使用较为成熟的网络库,
,,,使用Kernel-ByPass类新技术,
,,握手挥手过程优化,配置充足的端口范围,
,,,客户端角色最好别用bind,
,,,小心半连接、全连接队列溢出,
,,,适当减少握手重试,
,,,打开TFO,
,,,保持充足的可见描述符上限,
,,,如果握手频繁，请启用短连接改用长连接,
,,,TIME_WAIT优化,tcp_max_tw_buckets
,,,,tw_reuse/tw_recycle(4.12中已取消)
,第10章 容器网络虚拟化,veth设备对：类似loopback回环设备，区别是veth是一对儿虚拟的设备，一个发，另一个设备上就能收到数据,类似loopback回环设备,
,,,一个veth包括两个虚拟设备,
,,,一个发，另一个设备上就能收到数据,
,,网络命名空间,一台Linux上可以创建多个网络命名空间,
,,,每个网络命名空间都是一个逻辑上独立的协议栈,
,,,每个网络命名空间都有属于自己的路由表、socket、网卡等网络资源,
,,,每个物理网卡、虚拟设备都可以设置归属到某个网络命名空间下,
,,虚拟交换机bridge,veth的一头可以“插”在虚拟bridge上,
,,,bridge可以将不同网络命名空间中的veth设备打通,
,,,让多组veth可以相互通信,