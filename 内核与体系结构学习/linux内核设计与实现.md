# Linux内核简介
为什么UNIX系统如此强大，稳定，健壮？
- Unix很简洁，有非常明确的设计目的
- unix抽象了所有东西为文件，对数据和设备的操作接口统一。
- Unix内核和相关系统工具软件基于c语言编写，移植能力强。
- Unix的进程创建非常迅速。
- Unix提供一套非常简单且稳定的进程间通信元语。
**Linux内核和unix内核比较**：
Unix内核是一个不可分割的静态可执行库，必须以巨大，单独的可执行块的形式在一个单独的地址空间中运行。Linux和Unix基本都需要MMU的支持，现在基本所有硬件系统都具备内存管理单元。
**单内核与微内核比较**：
单内核即整个内核运行在单独一个地址空间，内核间的通信是微不足道的，可以直接调用内核所有函数，具有简单和性能高的优点，大多数unix系统为单内核模式。
微内核：不做为一个单独的大过程，微内核划分为多个独立过程，每个过程叫做服务器（CS架构）。不能直接调用函数，需要使用消息传递机制来处理内核通信：采用进程间通信机制来让各个服务器互通消息，互换“服务”。但是如果微内核的服务有些没有运行在内核，那么就需要额外的IPC开销，设计用户态和内核态的上下文切换，所以实际运用的微内核系统基本是让全部或大部分服务器位于内核。
==Linux也是单内核,但器规避了微内核设计的性能损失缺陷，所有的内核函数运行在内核态而无需消息传递。==
**Linux和Unix设计上的显著差异：**
- Linux支持动态加载内核模块。可在需要时动态的卸除/加载部分内核代码。
- Linux支持对称多处理SMP机制，传统UNIX基本不支持。
- Linux内核可以抢占，允许在内核运行的任务有优先执行的能力。
- Linux对线程的支持，（将进程和其他线程等同）
- Linux提供对设备类的面向对象的设备模型，热插拔事件，用户空间的设备文件系统。
- Linux忽略了unix的设计的很拙劣的特性
# 从内核出发
## **内核开发特点**：
- 不能使用c/c库/必须使用GNU c
- 内核编程缺乏用户空间那样的内存保护机制，难以执行浮点计算。
- 每个进程只有很小的定长堆栈。
- 内核支持异步中断，SMP，抢占。注意同步和并发。
- 考虑可移植性的重要性。
### **GNU** **C**：
内核开发者需要用到gcc提供的语言拓展部分。gcc是多种GNU编译器的集合。
- 内联函数：
消除函数调用和返回带来的开销，一般使用static关键字限制（其他编译单元不可见）。内联函数在头文件中直接定义。
- 内联汇编
即asm指令嵌入c中。
- 分支声明
gcc内建了一条指令likely和unlikely用于优化，当条件语句中，该条件很少出现，则使用unlikely标识，否则使用likely标识。
![image-20231117162723182](linux内核设计与实现.assets/image-20231117162723182.png)
### 无内存保护机制和不要轻易使用浮点数
内核发生内存错误会导致oops，主要是没有错误反馈，不知道哪里发生错误。
用户空间浮点操作，内核完成从整数操作到浮点数操作模式的转换。==执行浮点指令时，体系结构不同，内核选择不同==但在内核空间，其不能完美支持浮点操作，在内核使用浮点数，需要人工保存和恢复浮点寄存器，还需要一些额外琐碎的事。
### 容积小且固定的栈
每个处理器的内核栈是固定的
### 同步和并发/可移植性
内核很容易产生竞争条件，要求能并发的返回共享数据。常见解决方法是自旋锁和信号量，内核也需要注意移植问题，大部分c代码应该和体系结构无关。
## 小结
请克服对内核陌生的恐惧，冲冲冲！！！！
# 第三章 进程管理
## 3.1 进程
进程：处于执行期的程序。
线程：在进程中的活动对象。
进程提供两种虚拟机制：虚拟处理器和虚拟内存。线程间可以共享虚拟内存，但每个都拥有各自的虚拟处理器。
fork()调用有两个返回，一个返回到父进程，父进程继续执行（pid=0）;另一个返回到子进程（pid>0）。现代Linux内核中，fork()由clone()系统调用来实现。程序通过exit()退出，父进程通过wait4()系统调用查看子进程是否终结。进程退出后设置为僵死状态， 直到父进程调用wait()/waitpid()为止。
## 3.2 进程描述符及任务结构
内核的进程列表存放于：任务队列（双向循环链表）。链表表项是task_struct,即进程描述符（包含一个进程所需所有信息）。任务队列形式：
![image-20231121102240662](linux内核设计与实现.assets/image-20231121102240662.png)
### 分配进程描述符
每一个任务的thread_info结构存放在内核栈的尾端，其中的task域存放了指向改任的实际的task_struct指针。
![image-20231121104055867](linux内核设计与实现.assets/image-20231121104055867.png)
### 进程描述符的存放
内核通过唯一进程标识值/PID来标识每个进程。PID默认值是32768;可根据实际需要修改/proc/sys/kernel/pid_max来提高上限。
内核里，对任务的处理通常需要获取task_struct的指针。所以如何快速获取进程描述符的位置非常重要。有些可直接用专门寄存器来存放这个task_struct的指针（速度快）。对于x86来说，其是在thread_info结构里，内核栈尾端。
### 进程状态
进程描述符的state字段描述当前进程状态，系统中进程必然处于以下5种状态的一种。
- 运行，即进程可执行：正在执行或者在执行队列等待执行。
- 可中断，进程正在睡眠：进程正在睡眠（阻塞），等待指定条件发生。
- 不可中断，进程接收到信号也不会被唤醒或者准备投入运行，其他与可中断态相同。通常在进程必须等待时不受干扰/等待事件快要出现。（所以ps查看进程，表标记为D状态的进程无法被杀死，其不响应任何信号）
- 被其他进程跟踪的程序，例如ptrace跟踪调试程序。
- 停止，即进程停止运行，通常在接收到SIGSTOP，SIGTSTP,SIGTTIN,SIGTTOU等信号时。
![image-20231121110719468](linux内核设计与实现.assets/image-20231121110719468.png)
### 设置当前进程状态
内核设置某个进程状态：
```c
set_task_state(task,state);/*必要时设置内存屏障来强制其他处理器作重新排序(数据同步)*/
set_current_state(state)等价于set_task_state(task,state)
```
### 进程上下文
注意进程在用户空间和内核空间的上下文切换。 
### 进程家族树
所有进程都为PID为1的init进程的后代，系统中除init进程外都有父进程。task_struct的parent字段指向父进程的地址。还有children字段，指向子进程的链表。
init进程的进程描述符是作为init_task(进程家族树的根节点)静态分配的：
```c
struct task_struct* task;
for(task = current;task!=init_task;task=task->parent){  
}//访问到根节点即init进程描述符。
//任务列表的双向循环链表的结构可以通过简单方式遍历系统中的所有进程描述符。
```
## 进程创建
unix采用了进程创建拆解为：fork()和exec()函数的方式。fork()拷贝当前进程创建一个子进程。父/子进程的区别是:PID,PPID(这里子进程的PPID是父进程的PID)以及一些资源和统计量（挂起的信号等）。exec()函数读取可执行文件载入地址空间运行。
### 写时拷贝（COW）
linux的fork()采用写时拷贝页来实现。内核并不复制整个进程地址空间，父子进程共享同一个拷贝。只有在需要写入的时候，数据才会被复制，这样各个进程都有自己的拷贝。在之前都是以只读方式共享，这种技术使地址空间上的页的拷贝被推迟到实际发生写入的时候进行。所以这里fork()调用开销其实只有复制父进程的页表和给子进程创建唯一的进程描述符。
### fork()
fork()/vfork()/__clone()库函数需要根据各自的参数标志去调用clone()->do_fork()->copy_process()函数。
- 调用dump_task_struct()函数创建一个内核栈，thread_info结构，task_struct。和父进程完全相同。
- 检查新创建这个子进程后，系统进程数目没有超出限制。
- 子进程进程描述符内许多成员需要被清0或设为初始值。一些资源或者统计量为子进程自己的描述符成员。task_struct大多数数据依然未被修改。
- 子进程状态设置为TASK_UNINTERRUPTIBLE(不可中断的状态)，保证其不会运行。
- copy_process()调用copy_flag()更新task_struct的flags成员。PF_SUPERPRIV标志（超级用户权限）清零;PF_FORKNOEXEC标志被设置。
- 调用alloc_pid()为新进程分配一个有效的PID。
- 根据传递给clone()的参数标志，copy_process()拷贝或共享打开的文件，文件系统信息，信号处理函数，进程地址空间，命名空间等。一般情况下这些资源共享，其他资源如对每个进程不同则需要自己拷贝副本。
- copy_process()函数扫尾并返回一个指向子进程的指针。
回到do_fork()函数，新创建的子进程被唤醒并让其投入运行。内核倾向于让子进程先运行，以避免父进程先运行的写时拷贝的额外开销。
### vfork()函数
不拷贝父进程页表项，其他和fork()一样。子进程作为父进程的一个==单独的线程==在它的地址空间里运行，父进程被阻塞，直到子进程退出或执行exec()。vfork()函数流程：
- 调用copy_process()函数时，task_struct的vfork_done成员被设置为NULL。
- 执行do_fork()函数时，给定特别标志，则vfork_done指向一个特殊地址。
- 子进程开始执行后，父进程不是马上恢复执行，等待直到子进程通过vfork_done指针发送信号。
- 调用mm_release()函数时，该函数用于进程退出内存地址空间，并且检查vfork_done是否为空，不为空像父进程发送信号。回到do_fork()，父进程唤醒并返回。
### 线程在Linux的实现
Linux实现线程的机制特殊，没有准备额外的调度算法或者数据结构来标识线程。线程视作一个和其他进程共享某些资源的进程。每个线程都有自己的task_stuct(和其他子进程不同的是，线程和父进程共享某些资源，如地址空间)。
假设有一个包含四个线程的进程，对于专门的线程支持的系统，通常会有一个包含指向四个不同线程的指针的进程描述符（描述地址空间，打开文件等资源）。
Linux中则是创建4个进程并且分配4个普通的task_struct，建立这4个进程指定共享某些资源。
### 创建线程
调用clone()时传递一些参数标志指定需要共享的资源。其实和fork()差不多，但是父子两共享地址空间，文件系统资源，文件描述符和信号处理程序。
三种不同调用->调用clone()的参数变化：
![image-20231121153610120](linux内核设计与实现.assets/image-20231121153610120.png)
如下图，fork(),vfork(),以及线程的创建，通过设置clone()函数的下面的不同的共享资源的标志或者不同的flag来完成不同形式的进程创建。
![image-20231121154241546](linux内核设计与实现.assets/image-20231121154241546.png)
![image-20231121154248091](linux内核设计与实现.assets/image-20231121154248091.png)
### 内核线程
内核进程（如flush,ksoftirqd）和普通线程一样可被调度或者抢占，只在内核空间运行。ps -ef查看Linux的内核线程。内核线程由其他内核线程创建，内核从kthreadd内核进程衍生所有内核线程。
kthread_create函数从内核线程创建一个新的内核线程；但是新创建的内核线程是不可运行状态；通过wake_up_process()函数来明确唤醒它。kthread_run函数等价于kthread_create函数+wake_up_process函数。内核线程启动后的退出：do_exit()函数/kthread_stop()函数（参数是kthread_create创建的task_struct），
## 进程终结
进程的终结大部分靠do_exit()来完成：
- task_struct标志成员设置为PF_EXITING，删除任一内核计数器，记账信息输出。
- exit_mm()函数释放进程占用mm_struct，如果该地址空间没被其他进程共享，彻底释放他们。
- 调用set__exit()函数。进程排队等候IPC信号，否则；离开队列。
- 调用exit_files()/exit_fs()函数，递减文件描述符/文件系统数据的引用计数。引用计数降为0，则资源可以彻底释放。
- 把task_struct函数的exit_code成员中的任务退出代码置为由exit()提供的退出代码/去完成其他内核机制规定的退出工作。
- 调用exit_notify()向父进程发送信号，给子进程重新找养父（为线程组其他线程或init进程），设置进程状态为EXIT_ZOMBIE.
- do_exit()函数调用schedule函数切换到新进程。do_exit函数不会返回。
进程相关联的资源被释放掉后，进程不可运行并处于EXIT_ZOMBIE退出状态；它只占用内核栈，thread_info结构以及task_struct结构。进程存在的唯一目的是向它的父进程提供信息。父进程收到信息/通知内核无关信息后，进程所持有的内存被释放，归还系统。
### 删除进程描述符
父进程获得已终结的子进程的信息后/通知内核它并不关注那些信息后，子进程的task_struct结构才被释放。wait()一族函数用来挂起调用它的进程，直到有一个子进程退出，此时函数会返回该子进程的PID。函数释放进程描述符时：
- 调用__exit_signal()函数，该函数调用_unhash_process(),后面又调用detach_pid()从pidhash上删除进程，以及任务列表删除进程。
- _exit_signal()函数释放目前僵死进程的所有剩余资源，进行最终统计和记录。
- 进程为线程组最后一个进程，领头进程已死掉。release_task()需要通知僵死的领头进程的父进程。
- release_task()需要调用put_task_struct()释放进程内核栈和thread_info结构所占页，释放task_struct占用的高速缓存。
### 孤儿进程造成的进退维谷
父进程在子进程退出前退出，需要保证子进程能找到一个新的父亲。前面提到在当前线程组重新寻找父亲/init进程作为父进程。
do_exit()->exit_notify()->forget_orignal_parent()->find_new_reaper():
首先遍历进程所在的线程组（即在同一个父进程下的进程组合），返回为其找到进程；如果线程组无其他进程则返回init进程作为养父进程。
ptrace_exit_finish()函数也有新的寻父过程，这次是给ptraced的子进程寻找父亲。
遍历两个链表：子进程链表和ptrace子进程链表。当一个进程被跟踪，其临时父亲为调试进程（==多线程调试，临时父亲都是同一个调试进程？==）。此时父进程退出了，系统会为它和它的所有兄弟重新找父进程。只需要在单独的被ptrace跟踪的子进程链表中搜索相关的兄弟进程，而无需遍历整个系统。系统进程找到或设置了新的父进程，就不会出现驻留僵死进程的危险了。
# 第4章 进程调度
调度程序负责决定将哪个进程投入运行，何时运行以及运行多长时间。
## 多任务
划分为非抢占式多任务和抢占式多任务。
抢占式：时间片。优先级等都属于此种类型。
非抢占式多任务环境，除了进程自己主动停止运行，否则一直运行。
现代os基本采用抢占式的方式。
## Linux进程调度
以前的为O(1)调度算法，也称作大O调度算法（对于服务器等工作负载很理想，但对需要交互的程序表现不佳）；然而在后面的内核版本中改为了完全公平调度算法，即CFS。
## 策略
策略决定调度程序染让什么进程运行以及负责优化使用处理器时间。
### cpu和IO消耗型的进程
大多数的GUI程序属于IO密集型程序；对于cpu密集型则是大量占用cpu的工作时间。针对这两种情况的调度策略也不一样。对于cpu密集型，调度策略应该是减少调度频率，增加每次调度的执行时间。对于IO密集型应该增加其响应速度，增加调度频率。==并不绝对，存在程序同时是IO也是CPU密集型，例如字处理器，需要疯狂IO以及处理这个字。==Linux和Unix对IO密集型程序做了很多优化，即缩短响应时间。
### 进程优先级
Linux采用两种不同的优先级范围：
- nice值（-20到+19，默认为0;现在扩大范围了），nice越高，优先级越低。nice值在Unix上代表分配给进程的时间片绝对值，在Linux上，nice值代表时间片的比例。ps -el 命令的NI列
- 实时优先级，可配置（默认0-99）。值越高优先级越高。==任何实时进程的优先级都高于普通的进程（实时可调度程序通常高于，实时不可调度程序（如中断等）不直接参与调度，优先级或更低）==
### 时间片
任何长时间片将导致系统交互性能差，故默认时间片很短。对于Linxu的CFS调度算法，其不直接分配时间片给进程，而是将处理器的使用比划分给进程（故和系统的负载密切相关）；此外进一步，nice值还将作为权重进一步再调整这个处理器的使用比。
对一个进程进入可运行态，Linux的CFS调度器，其抢占先机决定于消耗了多少处理器使用比；**如消耗的使用比比当前运行进程小**，则可以立即抢占运行。
### 调度策略的活动
对于系统中的文字编辑程序和视频编码程序。我们希望调度器能更倾向于文字编辑程序。所以文字编辑程序应该有更多的处理器时间，以及在文字编辑程序唤醒时抢占视频编码程序。目标达成通过分配文字编辑程序更高优先级设置以及更多的时间片。对于Linux程序，当系统中只有两个程序且nice值也一样；那么处理器使用比都为50%。
关键是：文本编辑程序被唤醒时，CFS注意到它的处理器使用比为50%，但其程序运行时间比视频编辑小得多，（相同的处理器使用比，运行时间很小也会抢占当前进程或者某些os能识别出其是实时应用程序也会立即替换掉它）所以CFS就立即抢占视频编码程序。因为文字编辑程序没有消耗完给它的处理器使用比（即分配给他的时间片），下次还是会立即调用该文字编辑程序。
## Linux调度算法
### 调度器类
linux调度器以模块方式提供，允许不同类型进程针对性的选择调度算法。每个调度器只能调度自己范畴的进程且拥有一个优先级。os会按照调度器优先级遍历调度器类，选择最高优先级的调度器类来选择执行程序。
CFS是针对普通进程的；而后面有针对实时进程的调度类。
### Unix中的进程调度
现代进程调度系统有两个通用概念：进程优先级和时间片。通常高优先级进程也会被分配更多的时间片且运行频繁。**引出问题，矛盾点：**
- nice值映射到时间片，这个设计有问题的，即将每一个值映射到绝对的时间片。针对同一优先级的的两个进程，具有相同的时间片如10ms，那么这里的进程上下文切换为10ms一次。但是对同一高优先级的进程，时间片都为100ms,这里的进程上下文切换却是100ms一次。另外高优先级进程通常是交互进程，特点是快速响应，低处理器时间。但这个和nice值和时间片挂钩相矛盾。
- 设计相对nice值。unix的nice值的改变基于初始值，如0对应100ms;1对应95ms;也可能18对应10ms,19对应5ms。这个nice值的改变前后成倍数差距。
- nice值到绝对时间片的映射；绝对时间片需要定时器节拍。定时器节拍的相邻节拍时间差距决定了nice值对应的相邻时间片的差距。
- 即便某些高优先级的进程的时间片用尽了，但是我们还想要唤醒相关进程。所以需要一些特殊规则，获得更多的处理器时间，这需要额外的规则，损害了其他进程利益。
总结：问题二可以修改将nice值的几何增加解决，问题三可以采用新度量机制将nice值和时间片分离。CFS采用摒弃时间片改用使用处理器使用比方式来保证公平性。
### 公平调度
CFS出发点：进程调度效果如同系统具备一个理想中的完美多任务处理器。CFS设置了一个调度周期的目标，称作==目标延迟==。当可运行任务趋于无限，每个任务获得的处理器使用比接近0（切换开销太大了）.所以会设置一个最小粒度，unix中基本设置为1ms。此外nice值越大，对应的处理使用时间比权重越小。任何进程获得的处理器时间是他自己和其他所有可运行进程nice值的相对插值决定的，相对的处理器使用时间的权重比。
## Linux调度实现
主要关注CFS调度算法的四部分：
时间记账，进程选择，调度器入口，睡眠以及唤醒。
### 时间记账
即记录进程的运行时间做个记账。
- 调度器作为一个实体结构保存在进程描述符task_struct中。
- vruntime变量存放进程虚拟运行时间。包括程序已运行时间和剩下所需时间。其值是经过可运行进程的总数进行了加权的运行时间。
### 进程选择
CFS选择下一个运行进程时，选择一个最小的vruntime值的进程（即最小的处理器使用时间比）。CFS使用红黑树来组织可运行进程队列，可快速找到最小的vruntime的进程。
- 可运行进程都用红黑树存储（基于二叉搜索树），键值为vruntime,那么只需找到最左边的叶子节点即为最小值（一般最左边叶子节点已缓存到rb_leftmost，方便快速返回对应的调用进程）。==系统无可运行任务，调用idle task(空闲任务，保证cpu不空闲)==
- 向树中加入新进程时，除了其在树的最左边叶子节点（需要将rb_leftmost重新指向新加节点），其他就是正常插入，并更新平衡。
- 向树中删除进程时，主要是删除节点后的重新平衡以及删除最左边节点时，重新寻找最左边节点更新rb_leftmost.
### 调度器入口
进程调度入口是函数schedule()。
首先找到最高优先级调度类->获取该调度类的可运行队列->决定谁是该运行的进程。
==加速小技巧，当所有可运行进程数量等于CFS类的可运行进程数（全部是普通进程），省略掉选调度器这一步==
### 睡眠和唤醒
进程休眠则移出可执行进程的红黑树，放入等待队列；唤醒则是从等待队列到可执行队列。等待队列上有可中断任务和不可中断任务（忽略信号）处于休眠态。
- 等待队列
![image-20231129202936849](linux内核设计与实现.assets/image-20231129202936849.png)
如上图，1）首先调用宏创建一个等待队列的项，2）后面自己加入该队列（等待条件满足，执行唤醒操作）。3）prepare_to_wait变更进程状态（有必要则将进程加回到等待队列），4）状态为可中断，收到信号唤醒进程时，检查处理信号，再次确认为真退出while，否则继续schedule()并一直重复这一步操作等待信号。5）条件满足退出循环，最后移出等待队列。
- 唤醒
唤醒指定等待条件的等待队列的所有进程.
```c
static ssize_t inotify_read(struct file *file, char __user *buf,
			    size_t count, loff_t *pos)
{
	struct fsnotify_group *group;
	struct fsnotify_event *kevent;
	char __user *start;
	int ret;
	DEFINE_WAIT(wait);

	start = buf;
	group = file->private_data;

	while (1) {
		prepare_to_wait(&group->notification_waitq, &wait, TASK_INTERRUPTIBLE);

		mutex_lock(&group->notification_mutex);
		kevent = get_one_event(group, count);
		mutex_unlock(&group->notification_mutex);

		pr_debug("%s: group=%p kevent=%p\n", __func__, group, kevent);

		if (kevent) {
			ret = PTR_ERR(kevent);
			if (IS_ERR(kevent))
				break;
			ret = copy_event_to_user(group, kevent, buf);
			fsnotify_put_event(kevent);
			if (ret < 0)
				break;
			buf += ret;
			count -= ret;
			continue;
		}

		ret = -EAGAIN;
		if (file->f_flags & O_NONBLOCK)
			break;
		ret = -EINTR;
		if (signal_pending(current))
			break;

		if (start != buf)
			break;

		schedule();
	}

	finish_wait(&group->notification_waitq, &wait);
	if (start != buf && ret != -EFAULT)
		ret = buf - start;
	return ret;
}
```
## 抢占和上下文切换
由context_switch()函数来处理，有新进程选出来运行时，调用该函数：
- 虚拟内存从上一个进程映射切换到新进程
- 将上一个进程的处理器状态切换到新进程处理器状态（保存/恢复栈信息/寄存器信息）.其他与任何体系结构相关的状态信息，以每个进程为对象来管理保存。
![image-20231130172004492](linux内核设计与实现.assets/image-20231130172004492.png)
除了以代码形式来提醒调用schedule()，内核设置了need_resched标志是否需要重新执行一次调度。可用于高优先级进程进入可执行态（即当前进程应该被抢占时），都设置该标志。
==在返回用户空间以及从中断返回的时候，内核也会检查need_resched标志，决定是否重新调度==。通常进程描述符里存储有need_resched，目的是快速获取该值。
### 用户抢占
内核返回用户空间时，即在系统调用/中断程序返回后，需要检查need_resched标志。
用户抢占发生时机：
- 从系统调用返回用户空间
- 从中断处理程序返回用户空间
### 内核抢占
支持在内核中运行的进程被抢占，为此引出了可抢占内核任务和不可抢占的任务。引入变量preempt_count.任务引入锁，计数加一；相反，计数减一。当该值为0，则是可抢占的，因为不占用锁；不为0，则当前任务不可抢占。preempt_count=0时，进程释放锁的代码检查need_resched是否设置，是则内核将被调度程序抢占。
内核进程阻塞或者显示调用schedule()函数，内核抢占显式的发生。
内核抢占发生时机：
- 中断处理程序正在执行，返回内核空间前。（==这意味着中断可以被中断，即中断嵌套==）
- 内核代码具有可抢占性，preempt_count=0
- 内核任务显式调用schedule()
- 内核任务阻塞
## 实时调度策略
Linux提供:SCHED_FIFO，SCHED_RR两种实时调度策略。SCHED_FIFO实现了一种简单，先入先出的调度算法。其比任何处于SCHED_NORMAL的进程优先级高。处于SCHED_FIFO的进程没有基于时间片，其只能自己阻塞。显式释放处理器或有更高的SCHED_FIFO或者SCHED_RR优先级进程；其才会让出CPU。SCHED_RR处理是基于时间片的外，其他和SCHED_FIFO差不多。
Linxu的实时性是软实时，只是尽量保证在时间内完成该任务，不保证一定完成。
实时优先级范围为[0,MAX_RT_PRIO(默认值为100)),SCHED_NORMAL默认进程的nice值共享这个取值空间。-20到+19对应100到139实时优先级范围
## 与调度相关的系统调用(编写为c库，可直接调用)
![image-20231201095913640](linux内核设计与实现.assets/image-20231201095913640.png)
### 与调度策略和优先级相关的系统调用
nice()函数在给定进程的静态优先级增加一个给定量。超级用户才能在调用它时使用负值来提高进程的优先级。
### 与处理器绑定有关的系统调用
linux调度提供强制的处理器绑定机制，通过task_struct的cpus_allowed这个位掩码标志，掩码每一位对应一个系统的可用处理器。
实现强制处理器绑定的方法，处理进行第一次创建时，继承父进程的相关掩码。故父子进程都运行在相应cpu上；处理器绑定关系改变，内核将任务推到合法处理器上，加载平衡器则把任务拉到允许的处理器上。
### 放弃处理器时间
linux通过sched_yield()系统调用，可让进程显式的将处理器时间让给其他等待执行进程的机制。普通进程时将进程从活动队列放到过期队列(暂时不会被调度);并将其放在优先级队列最后面。对于实时进程，其只会放在优先级队列的末尾。
# 第五章 系统调用
## 与内核通信
系统调用在用户空间进程和设备间的一个中间层。负责用户进程和内核间的通信。
## API，POSIX和c库
应用程序通过调用c库来使用系统调用一般。而c库中的API一般是基于POSIX标准。
==底层的设计可以参考unix的接口设计名言->**提供机制而不是策略**==
## 系统调用
对于getpid()系统调用，内核实现：
![image-20231201104439460](linux内核设计与实现.assets/image-20231201104439460.png)
```c
asmlinkage long sys_getpid(void)
//上述系统调用到内核实现后，其展开是上面这个函数
//samlinkage是限定词，即仅从栈中提取函数参数，所有系统调用展开后都需要该限定词。
```
### 系统调用号
系统调用号一经分配就不会变更。当一个系统调用被删除时，其所占用的系统调用号也不能删除。这里linux有另一个系统调用sys_ni_syscall()，除了返回-ENOSYS外其他不做操作。其针对删除的系统调用，填补其空缺。内核具有一个系统调用表。存储系统调用号和系统调用的对应关系。
### 系统调用性能
linux上下文切换时间断是linux执行效率高的重要原因。
## 系统调用处理程序
通过软终端int $0x80陷入内核。系统调用号通过eax寄存器传递给内核。sys_call()函数检查系统调用号有效性，是否大于等于NR_syscalls。
系统调用函数参数传递，ebx,ecx,edx,esi,edi来顺序存储五个参数。返回值存放在eax寄存器中。
## 系统调用实现
确定用途->尽量单一用途->参数尽量少,即简洁高效->考虑通用性，不对函数作不必要限制->考虑系统调用的可移植性和健壮性。
### 参数验证
系统调用需要检查所有参数看是否合法。最重要的是检查用户提供指针的有效性。即保证：
- 指针指向内存属于用户空间。
- 指针指向地址在进程地址空间，进程绝不能让内核读其他进程数据。
- 读/写/可执行，内存需对指针标记这些访问限制。
如下图所示：
![image-20231201142826402](linux内核设计与实现.assets/image-20231201142826402.png)
copy_to_user()和copy_from_user();为内核向用户空间地址写入数据以及从用户空间数据拷贝到内核这两个系统调用。第一个参数为拷贝目标地址，第二个参数为源地址，第三个为拷贝数据量。
对每个系统调用函数，在允许该系统调用前，需要检查进程是否有使用该系统调用的权力。
如capable(CAP_SYS_NICE)/capable(CAP_SYS_REBOOT)用于检查该进程是否有修改其他进程nice值和reboot的权限。
## 系统调用上下文
current指针指向当前进程的上下文，由于进程是可以被抢占的，为了防止新进程调用上个进程的相同的系统调用不出错，必须保证系统调用是可重入的。
### 绑定一个系统调用的最后步骤
- 系统调用表最后加入表项，下表为其系统调用号。
- 各种体系结构，系统调用号定义于<asm/unistd.h>中
- 系统调用不能是模块，只能编译进内核
将该系统调用加入到<asm/unistd.h>中，有新的系统调用编号。然后将实现的源文件放到kernel目录下，保证其被编译进内核。这样用户空间就可以调用该系统调用。

### 用户空间访问系统调用
![image-20231201150826630](linux内核设计与实现.assets/image-20231201150826630.png)
如上图所示，宏定义扩展成内嵌汇编的函数，不用库来支持。后面的_syscall0(long,foo)，主要是压入参数到栈中，方便系统调用允许。用户程序可直接使用系统调用函数名来达到自己的功能。
### 为什么不通过系统调用方式实现
尽量不新建系统系统调用实现功能，而应该想其他解决方法。
新建系统调用好处：
- 系统调用创建容易且使用方便。
- linux系统调用性能高。
坏处：
- 需要一个开放的系统调用号，系统调用加入就固化了，避免app崩溃，接口不允许改动。
- 系统调用分别注册到需要的支持的体系结构中去，脚本不容易调用系统调用，也不能文件系统直接使用
- 需要系统调用号，在内核树外很难维护。
**原则是linux系统尽量避免添加新的系统调用，而是复用已有系统调用**
# 第6章 内核数据结构
内核通用数据结构:
## 链表
链表是linux内核中最简单的数据结结构。
双向链表
### 环形链表
环形双向链表是最灵活的，所以linux内核的标准链表是环形双向链表（）。
一般首元素为头指针，方便寻找链表起始端。
### linux内核中的实现
- 链表数据结构
```c
struct list_head{
	struct list_head *prev;
	struct liast_head *next;
};
struct fox{
	unsigned long tail_length;
	unsigned long weight;
	bool is_fantastic;
	struct list_head list;
}
//可以用list.prev/list.next访问前后元素。提供list_entry()内置函数来增删改查。
```
- 定义一个链表
运行时创建和静态创建：
![image-20231201163431737](linux内核设计与实现.assets/image-20231201163431737.png)
- 链表头
```c
static LIST_HEAD(fox_list);//定义并初始化一个链表例程。
```
### 操作链表
- 向链表增加一个节点
list_add(struct list_head *new,struct list_head *head);
将new节点数据添加到head后。head为最后一个元素，则可实现栈。head为第一个元素，则实现为队列。
- 删除节点
list_del(struct list_head *entry);
删除该节点并改变其前后节点。
另外：从链表中删除节点数据并且保留：
list_del_init();
list_del_init(struct list_head *entry);
把节点从一个链表移到另一个链表
- 移动和合并链表节点
list_move(struct list_head *list,struct list_head *head) 
从一个链表中移除list，将其加入到另一链表的head节点后面
list_move_tail(struct list_head *list,struct list_head *head)
将list项插入到head前。
list_splice(struct list_head *list,struct list_head *head);
将list指向的链表插入到指定链表的head元素后面。
list_splice_init(struct list_head *list,struct list_head *head)
list指向的链表重新初始化，其他和list_splice一样。
节约两次提领：
已经得到了节点的前后指针。只需要调用内核相应的加双下划线的同名函数即可。
这里就是节约了获取两个变量的时间，即节约两次提领。
### 遍历链表
```c
//遍历链表用法
struct list_head *p;
struct fox *f;
list_for_each(p,&fox_list){
	f=list_entry(p,struct fox,list);
}
//其他封装的函数
list_for_each_entry(f,&fox_list,list){//从f开始沿next开始遍历
}
//反向遍历
list_for_each_entry_reverse(f,&fox_list,list){//从f开始沿prev遍历
}
//遍历且删除
list_for_each_entry_safe(pos,next，head，member){
}
list_for_each_entry_safe_reverse(pos,next，head，member){
}
```
## 队列
### kfifo
kfifo对象维护了：入口偏移和出口偏移两个变量。入队从入口偏移处入队；出队从出口偏移处出队。==出口偏移总小于等于入口偏移，入口偏移最大为队列长度，此时队列满==
### 创建队列
即对kfifo对象进行定义和初始化。
数据入队：
unsigned int kfifo_ in(struct kfifo *fifo, const void *from，unsigned int len) ;
数据出队：
unsigned int kfifo_ _out (struct kfifo *ifo, void *to， unsigned int len);
希望数据出队而出口偏移不增加：
unsigned int kfifo_out_peek(struct kfifo *fifo， void *to，unsigned int len,unsigned offset);
其他还可获取kfifo队列长度，重置队列（数据清空）以及撤销队列（kfifio_free()操作释放kfifo_alloc()分配的队列）
## 映射
关联数组，键值组成，每个键关联一个特定值。
c++中的std::map便是采用自平衡二叉搜索树（红黑树，AVL树等）
linux内核的映射目标：映射一个唯一的标识数（UID）到一个指针。这个映射叫做idr。
- 首先分配一个唯一的uid，保证uid不被重用，而且将指针和这个uid关联起来。
- 查找uid即idr_find();删除uid，撤销idr（只释放idr未使用内存，不释放分配给uid的任何内存）
## 二叉树
### BST二叉搜索树
### 自平衡二叉搜索树
即所有叶子节点深度差不超过1.
#### 红黑树
linux主要的平衡二叉树数据结构是红黑树。
- 节点或红或黑；叶子节点为黑色且没有数据
- 所有非叶子节点有两个子节点，一个节点红色，子节点必为黑色，一个节点到叶子节点的路径中，包含同样数目的黑色节点，该路径相对其他路径最短（即所谓的黑高度相同）。
==红黑树如何保证最深的叶子节点的深度不会大于两倍的最浅叶子节点的深度，红黑树的插入和删除操作==
证明：路径最短是全是黑色节点的路径高度，即最浅叶子节点深度为h。而由于最深是红黑交替（不能为红连续），所以最多为2h得证。
#### rbtree
即linux实现的红黑树，保证了平衡性，插入效率和节点数目呈对数关系。
一般来说搜索和插入由用户来实现。
## 数据结构的选择
链表：遍历操作，存储较少数据，性能不优先，和内核其他链表交互。数据量不定选链表。
队列：生产者/消费者模式，定长缓冲选队列。
映射：处理发给用户空间的描述符，需要映射一个uid到一个对象。
红黑树：存储大量数据，检索迅速。
**请优先考虑内核以及实现的数据结构，实在没退路再自主设计**
## 算法复杂度
**时刻注意算法的负载和典型的输入集合大小关系**，**不要为了不需要支持的伸缩度需求而盲目优化算法**。
# 第7章 中断和中断处理
## 中断
涉及中断信号->中断控制器（和处理器相连接管线与处理器通信）->中断请求线（IRQ）对应着不同的中断值。
中断可以是动态分配的，**重点是特定中断和特定设备联系**。
异常：缺页异常，除0异常等。
## 中断处理程序
保证中断处理程序的快速执行，尽可能快恢复中断代码执行；同时还需要完成该完成的工作（例如网络收报，字符响应等）。
于是中断分为上下两个半部。
禁中断的情况下，处理中断上半部（中断应答，硬件复位）.（例如网络上半部需要将数据从网卡发给内存）
开中断情况下，在合适的机会，下半部会执行。（网络下半部则是处理这个数据）
## 注册中断处理程序
![image-20231206174525425](linux内核设计与实现.assets/image-20231206174525425.png)
irq：分配的中断号。传统PC设备，这个值固定，大多数设备靠探测或编程动态确定。（中断控制器收到中断信号，根据传统或动态的方式确定中断号，将中断信号通过和处理器相连接的引脚发送，处理器解析信号得到对应中断号->中断向量表，往后走）
handler：指针，指向实际的中断处理程序。
flags：中断处理程序标志
- IRQF_DISABLED，内核在处理中断处理程序期间，禁止所有的其他中断。
- IRQF_SAMPLE_RANDOM,将其作为内核熵池一员，确保设备工作时间不确定或不受攻击。
- IRQF_TIMER,系统定时器中断准备。
- IRQF_SHARED,多个中断处理程序之间共享中断线，同一irq线注册的每个处理程序需要指定这个标志。
name:与中断相关的设备的ASCALL码的文本表示，PC机上键盘中断就对应"keyboard"。
dev:用于共享中断线，这个参数用于辅助内核直到给定中断线上应该删除哪个处理程序。实践中往往传递驱动程序的设备结构（唯一）。
注意request_irq()会引起阻塞，不能放在不被允许阻塞的代码中。本质上是因为在注册过程中，内核需在/proc/irq文件中创建对应项，最终需要通过kmalloc()来请求分配内存。kmalloc()是可以睡眠的。
以下是一个中断注册的实例：
![image-20231207155204985](linux内核设计与实现.assets/image-20231207155204985.png)
**释放中断处理程序：**
在卸载驱动程序时，也要删除对应的中断处理程序。
void free_irq(unsigned int irq,void *dev)
指定中断线不共享，删除处理程序同时禁用这条中断线。共享的则是仅删除dev对应的处理程序。
## 编写中断处理程序
![image-20231207160812995](linux内核设计与实现.assets/image-20231207160812995.png)
如上中断处理程序必须与request_irq()中handler所要求参数类型相匹配。dev参数对应着唯一的设备，可区分来自同一中断处理程序的多个设备。大多数中断处理程序需要知道产生中断的设备
重入和中断处理程序，**linux中的中断处理程序是不需要重入的**。因为当中断处理程序执行时，相应的中断线会被屏蔽掉，所以不会再响应该中断线的中断，所以无需涉及中断处理程序是可重入的。
### 共享的中断处理程序
共享和非共享的中断处理程序的差异性：
- request_irq()参数flags必须设置IRQF_SHARED标志。
- 共享中断处理程序的dev不能传递NULL值，通常选择设备结构传递。
- 共享中断处理程序需要能区分他的设备是否发生中断，需要硬件支持和处理程序相关逻辑实现。
所有共享中断处理程序应该遵循上面三条。
内核收到一个中断，依次调用该irq上注册的每个处理程序。中断处理程序需要通过硬件设备提供的状态寄存器（其他机制）检测是否该为此次中断负责。
## 中断上下文
中断上下文是不允许被睡眠（中断上下文里不能放入阻塞代码），顶多只是中断嵌套，当前中断上下文处于运行队列排队，而不是被阻塞掉。正是中断处理程序打断了代码执行的特点，它必须是迅速的，简洁的。需要将大量具体工作从中断处理程序中分离出来，放到下半部去执行。下半部会找一个更合适的时间执行。
中断处理程序可以拥有自己的栈，虽然比较小，但是是自己拥有的，而不是以前的去和被中断进程的内核栈来共享。
## 中断处理机制实现
对每条中断线，处理器跳到对应的唯一的位置（irq和dev确定）。计算出中断号后，进行中断应答后，就会禁用这条线的中断。该条中断线上的每一个潜在的中断处理程序都要运行，不共享则只执行一次，否则循环依次执行潜在处理程序，找到和dev对应的中断处理程序执行。找到对应的中断处理程序执行完成后，再执行ret_from_intr()返回。如返回用户空间，检测need_resched标志。是否需要被重新schedule()。如返回内核空间，只在preempt_count为0（没有锁）时，再调用schedule()。
## /proc/interrupts
procfs是一个虚拟文件系统，只存在于内核内存。我们常cat /proc/...下的文件，来查看内核内存的现状，从而分析。
## 中断控制
控制中断系统的原因是需要提供同步，通过禁止中断，可确保某个中断处理程序不会抢占当前的代码。而锁机制通常是为了防止SMP机制导致的其他处理器的并发访问。获取锁的同时伴随着禁止本地中断。（**这里我的看法是为了让该进程获取锁后不被中断，防止其他等待锁的进程过多等待**）
### 禁止和激活中断
禁止当前处理器的本地中断，即：
```c
local_irq_disable();
local_irq_enable();
```
问题：在调用local_irq_disable()之前已经禁止了中断，该例程带来危险性。同理对local_irq_enable()也一样。于是我们可以采用保存中断状态的办法。
```c
unsigned long flags;
local_irq_save(flags);
...
local_irq_restore(flags);
//flags包含中断系统状态，包括栈帧信息，flags不能传递给另一个函数，所以只能在同一函数中使用这两个函数。
```
### 禁止指定中断线
![image-20231207191200523](linux内核设计与实现.assets/image-20231207191200523.png)
如上图四个函数用于禁止指定的中断线。
disable_irq()需要所有的中断处理程序全部退出后才能返回。disable_irq_nosunc()则必须要等待。enable_irq()和前面两个函数对应，用于激活中断线。synchronize_irq()等待一个特定的中断处理程序的退出，只在该函数退出后才返回。
### 中断系统的状态
获取内核中断系统的状态，是否处于中断？
![image-20231207191958240](linux内核设计与实现.assets/image-20231207191958240.png)
# 第8章 下半部和推后执行的工作
整个中断流程分为两半：第一部分是中断处理程序（需要快速，异步，简单的·机制负责对硬件做出迅速响应）。第二部分是下半部。
## 下半部
上下半部如何划分工作：
- 任务对时间敏感->放在中断处理程序执行。
- 任务和硬件相关->放在中断处理程序执行。
- 任务保证不被其他中断（特别是相同中断）打断，放在中断处理程序。
- 其他任务则是放在下半部。
### 为什么下半部？
一些中断处理程序可能有IRQF_DIASABLE标志，禁用所有中断，最差也会禁用同级中断。这样需要中断处理程序快速完成->将一些工作分离出来到以后去做，即下半部的工作。==运行下半部时，允许响应所有的中断==
内核提供三种下半部实现机制：**软中断，tasklet，工作队列**。此外还有内核定时器机制（保证操作推迟到某确定时间段执行）
## 软中断
### 软中断实现
软中断在编译时静态分配，有一个软中断的数组，包含32项，每个软中断需要注册到该数组中。即static struct softirq action softirq _vec[NR_ SOFTIRQS];网络的软中断注册就用到了。
一个软中断不会抢占另一个软中断。只有中断处理程序可以抢占软中断。
执行软中断：注册的软中断只有在被标记后才能执行即触发软中断。
- 从硬件中断代码处返回时，如网卡硬中断。
- 在ksoftirq内核线程中，显式检查/执行待处理的软中断代码中。
软中断处理核心：
![image-20231208162204271](linux内核设计与实现.assets/image-20231208162204271.png)
- pending是32位位图，对应对应的softirq_vec。待处理的软中断位图被保存，实际的软中断位图清0（这里需要屏蔽中断，避免在清0间隙，新的软中断导致该软中断被清0）
- pending依次右移，逐步处理对应位为1的softirq_vec中的对应的软中断处理函数。
### 使用软中断
只有两个子系统网络和SCSI直接使用软中断，因为对时间要求严格以及下半部重要性。内核定时器和tasklet建立在软中断基础上。
- 分配索引，在下面的枚举类型中加入新的项
![image-20231208163514815](linux内核设计与实现.assets/image-20231208163514815.png)
- 注册处理程序，open_softirq()注册软中断处理程序。软中断护理程序执行时，当前处理器的软中断被禁止，其他处理器可执行别的软中断（相同的也可）。以为这软中断处理程序的共享数据需要严格的锁保护，**所以大部分的软中断处理程序都采取单处理器数据（即不是多处理器共享数据，无需加锁）来提升性能**。
如果不需要多处理器扩展性，请使用tasklet，它同个处理程序的多个实例不能在多个处理器同时允许。
- 触发软中断，raise_softirq(NET_TX_SOFTIRQ),可在下次调用do_softirq()函数时投入运行。
## tasklet
通常使用tasklet而不是软中断。
### tasklet的实现
两类软中断代表：HI_SOFTIRQ>TASKLET_SOFTIRQ.
- tasklet结构体
![image-20231208170919585](linux内核设计与实现.assets/image-20231208170919585.png)
如上图，state在0，run，sched三者间取值，表示tasklet是被调度还是运行。count为计数器，不为0则tasklet禁止否则tasklet被激活，等待被挂起执行。func是tasklet的处理程序，data为其唯一参数。
- 调度tasklet
已调度的taasklet存放在两个单处理器数据结构：tasklet_vec(普通的tasklet)/tasklet_hi_vec（高优先级的tasklet）.两个数据结构都是由tasklet_struct结构体构成的链表。
由tasklet_schedule()和tasklet_hi_schedule()（高优先级，即HI_SOFTIRQ）进行调度.
tasklet_schedule()的执行步骤：
- 检查tasklet的state，为sched则立即返回，因为已经调度过了。
- 调用_tasklet_schedule(),保存中断状态，禁止本地中断。
- 把需调度的tasklet放到链表（tasklet_vec,tasklet_hi_vec）中去.
- 唤醒HI_SOFTIRQ/TASKLET_SOFTIRQ软终端，下次调用do_softirq()则执行该tasklet。

tasklet_action()和tasklet_hi_action()做了什么？
- 禁止中断，检索两个链表
- 把当前处理器的该链表置为NULL，来清空。
- 允许响应中断，循环遍历链表上每一个待处理的tasklet
- 多处理器系统，检查tasklet_state_run标志来判断这个tasklet是否在其他处理器运行，是的话现在不能执行，找下一个tasklet。
- 检查count是否为0，不为0，则禁止跳到下一个。否则开始执行，设置state为run，tasklet运行完，清除state的run标志。
- 重复执行tasklet，知道没有剩余的。
### 使用tasklet
- 可静态/动态的创建一个tasklet
![image-20231208174647998](linux内核设计与实现.assets/image-20231208174647998.png)
上面两个宏用于静态创建一个tasklet_struct结构。区别在于引用计数初始值不同，第一个为0，已激活的tasklet，第二个为1，为禁止的tasklet。
下面是动态创建：
![image-20231208174929570](linux内核设计与实现.assets/image-20231208174929570.png)
- 编写自己的tasklet的处理程序
tasklet的处理程序靠软中断实现，不允许睡眠（本身就是要处理程序尽快处理完成，如果允许等待事件而阻塞，这将大大延长其处理时间，和初衷相违背）。
- 调度自己的tasklet
调用tasklet_schedule()函数传递相应的tasklet_struct指针。**在tasklet被调度但未运行，又被调度，只运行一次。否则，已经运行则是会被再次调度运行。**如下图：
![image-20231208190920875](linux内核设计与实现.assets/image-20231208190920875.png)
可以禁止某个tasklet，也可以激活某个tasklet。
- ksoftirqd
每个处理器会有一个协助处理软中断的内核线程。对于重新触发的软中断，是立即处理还是先完成此次处理（用户任务可能得不到执行），下一次软中断执行再一起处理（软中断得不到即时处理）。折中方案：==不立即处理重新触发的软中断；大量软中断出现时，内核唤醒一组内核线程来进行处理（最低优先级运行）==。每个处理器一个ksoftirqd内核线程，只要有待处理的软中断，ksoftirq调用do_softirq()进行处理。
## 工作队列
推后执行的任务需要睡眠，选择工作队列，否则选择tasklet/软中断。
### 工作队列的实现
工作队列子系统创建内核线程，创建的线程可用于执行内核其他部分排到队列的任务。工作队列子系统提供缺省的工作者线程（events/n，n为处理器编号）来处理需要推后的工作。对于cpu密集型或者性能要求高的推后任务，可以自己创建工作者线程来进行处理。
- 工作者线程表示：
  ![image-20231208194511240](linux内核设计与实现.assets/image-20231208194511240.png)
  每个工作者线程类型分配了一个cpu_workqueue_struct，即每个处理器都有一个这样的工作者类型的线程。

- 表示工作的数据结构
  ![image-20231208195012480](linux内核设计与实现.assets/image-20231208195012480.png)
  每个处理器每种类型的这个工作的数据结构连成一个链表，对应的工作者类型的线程被唤醒时，处理各个处理其上的对应类型的工作链表，处理完一个work_struct后就将其移除链表。
  工作者线程要执行的work_thread()函数：
  ![image-20231208195527758](linux内核设计与实现.assets/image-20231208195527758.png)
  run_workqueue()函数会将循环遍历链表上每个待处理的工作，调用work_struct的func函数。

- 工作队列实现机制总结
![image-20231208211640195](linux内核设计与实现.assets/image-20231208211640195.png)
如上图，一类工作类型workqueue_struct，包含了一个cpu_workqueue_struct;即每个处理器都有自己的工作者线程。在底层是不同工作类型组成的多个work_struct链表，对应的工作线程遍历该处理器对应的work_struct来完成对应的延迟工作。
### 使用工作队列
- 创建推后工作，即work_struct
![image-20231208212340059](linux内核设计与实现.assets/image-20231208212340059.png)
![image-20231208212406228](linux内核设计与实现.assets/image-20231208212406228.png)
静态/动态创建延迟的工作
- 工作队列处理函数
处理函数会由工作者线程执行，运行在进程的上下文中
- 对工作进行调度，schedule_work(&work)会立即调度work，当所在处理器上的工作者线程被唤醒，就会执行work的func。
- 刷新操作，有时在继续下一步工作前，需保证一些操作已经执行完毕。使用void flush_scheduled_work(void),函数等待直到工作队列的所有对象执行返回。
- 创建新的工作队列，
![image-20231208213700542](linux内核设计与实现.assets/image-20231208213700542.png)
如上创建一个新的工作队列和与之对应的处理器线程。（各个处理器的同类型的工作者线程去全局该类型的工作队列获取对应的任务）
## 下半部机制的选择
需要反应迅速，执行频率高的应用选择软中断。
需要同类型中断多处理器只能运行一个tasklet，选择tasklet。
延迟的任务需要阻塞或者处于进程上下文中，那么选择工作队列。
易用性而言：工作队列>tasklet>软中断
![image-20231208214851010](linux内核设计与实现.assets/image-20231208214851010.png)
